{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generate + Filter data in the same File"
      ],
      "metadata": {
        "id": "3aQ9Yb_PTz0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark_release='spark-3.4.2'\n",
        "hadoop_version='hadoop3'\n",
        "\n",
        "import os, time\n",
        "start=time.time()\n",
        "os.environ['SPARK_RELEASE']=spark_release\n",
        "os.environ['HADOOP_VERSION']=hadoop_version\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_release}-bin-{hadoop_version}\""
      ],
      "metadata": {
        "id": "ZgAT-64nTzf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker pysqlite3\n",
        "!pip install mysql.connector\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "-2DQkwDEbV-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b59bd92e-7ef4-4628-a47e-1eeba3a8db61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faker in /usr/local/lib/python3.10/dist-packages (22.0.0)\n",
            "Requirement already satisfied: pysqlite3 in /usr/local/lib/python3.10/dist-packages (0.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
            "Requirement already satisfied: mysql.connector in /usr/local/lib/python3.10/dist-packages (2.2.9)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark"
      ],
      "metadata": {
        "id": "bnJ-4Q8NyMNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run below commands in google colab\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # install Java8\n",
        "!wget -q http://apache.osuosl.org/spark/${SPARK_RELEASE}/${SPARK_RELEASE}-bin-${HADOOP_VERSION}.tgz # download spark-3.3.X\n",
        "!tar xf ${SPARK_RELEASE}-bin-${HADOOP_VERSION}.tgz # unzip it\n",
        "!pip install -q findspark # install findspark"
      ],
      "metadata": {
        "id": "wl8IziHQycfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "import multiprocessing\n",
        "import pyspark\n",
        "import socket\n",
        "import uuid\n",
        "import findspark\n",
        "from pyspark.conf import SparkConf\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import expr\n",
        "from pyspark.sql.streaming import DataStreamReader\n",
        "\n",
        "import sqlite3\n",
        "from faker import Faker\n",
        "import random\n",
        "import datetime\n",
        "import json\n",
        "fake=Faker()"
      ],
      "metadata": {
        "id": "0ednyWm-bt52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# findspark find your Spark Distribution and sets necessary environment variables\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "tc2xPTOJyee0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the pyspark version\n",
        "print(pyspark.__version__)"
      ],
      "metadata": {
        "id": "7h-gacbeygMT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ebdfe8c-1ddd-42e0-f4a7-2c5a5f35a1a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DStream that will connect to hostname:port, like localhost:9999\n",
        "# if doing this over a network, firewalls may block the connection!\n",
        "hostname=socket.gethostname()\n",
        "\n",
        "hostname"
      ],
      "metadata": {
        "id": "N1G55x3fymTE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58ca0c24-b03f-4c7f-b693-f127a671ee06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'05a220fe72d3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "app_id=str(uuid.uuid1())\n",
        "\n",
        "app_id"
      ],
      "metadata": {
        "id": "qjL2UCxoyoBy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82feb13e-6f90-4d4e-83e7-d7436876b7a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2173ace4-a9bd-11ee-9fbc-0242ac1c000c'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conf = SparkConf()\n",
        "\n",
        "conf.setAll([\n",
        "     ('spark.app.name', app_id),\n",
        "     ('spark.shuffle.useOldFetchProtocol', 'true'),\n",
        "     ('spark.testing', 'true'), # Avoid minimum 450M executor/driver memory https://www.waitingforcode.com/apache-spark/troubleshooting-system-memory-must-be-at-least-error/read / https://programmerclick.com/article/72821685476/\n",
        "     ('spark.driver.allowMultipleContexts','true'), # https://stackoverflow.com/a/41591258 This option is used only for Spark internal tests and is not to be used in production.\n",
        "     ('spark. y', '100M'),\n",
        "     # ('spark.driver.memory ', '200M'),\n",
        "     # ('spark.executor.instances',1), # This property is no longer used in Spark 2+\n",
        "     # number of executors is determined as: floor(spark.cores.max / spark.executor.cores)\n",
        "     (\"spark.executor.cores\",1), # cores per executor. https://stackoverflow.com/questions/39399205/spark-standalone-number-executors-cores-control/39400195#39400195\n",
        "     (\"spark.cores.max\", 2), # the maximum amount of CPU cores to request for the application from across the cluster (not from each machine)\n",
        "     ('spark.submit.deployMode', 'client'), # client, cluster\n",
        "     ('spark.ui.showConsoleProgress', 'true'),\n",
        "     (\"fs.azure\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\") ])"
      ],
      "metadata": {
        "id": "8LlJ3BHFyogz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9882af1b-2d51-41b1-91a0-c2efdbe8c031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.conf.SparkConf at 0x7eca804c7490>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "end=time.time()\n",
        "\n",
        "\n",
        "f'Spark setup time: {int(end-start)} seconds'"
      ],
      "metadata": {
        "id": "_J85b_QsysNI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "855ffc94-923c-41b1-cf17-d0caf4c845f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Spark setup time: 115 seconds'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip mysql-connector-j-8.2.0.zip"
      ],
      "metadata": {
        "id": "yg-xyAr7IiG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set up directories for later usage\n",
        "!mkdir glucose_readings_dir device_readings_dir alerts_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRCbpQLG1WYH",
        "outputId": "6c760abc-392d-4266-bcaa-c3265f7887d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘glucose_readings_dir’: File exists\n",
            "mkdir: cannot create directory ‘device_readings_dir’: File exists\n",
            "mkdir: cannot create directory ‘alerts_dir’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Data\n",
        "1. Take patient data stored in MySQL (e.g. ID)\n",
        "2. Generate Glucse Readings Data — use timesynth library for better time-dependency of health data\n",
        "3. Generate Device Readings Data — use Faker"
      ],
      "metadata": {
        "id": "UDCHiW5AiZFx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Device Data\n",
        "\n",
        "1. Random Firmware Version Function: This function generates a random firmware version number. It creates a version string in the format major.minor.patch, where each part is a random integer (major is between 1 and 3, minor and patch are between 0 and 9).\n",
        "\n",
        "2. Random Connectivity Status Function: This function randomly selects a connectivity status for a device from three options: 'Connected', 'Disconnected', or 'Poor Connection'.\n",
        "\n",
        "3. Random Error Codes Function: This function randomly decides whether to assign an error code to a device. If an error code is to be assigned, it randomly selects one from 'Err1', 'Err2', 'Err3', or an empty string (representing no error).\n",
        "\n",
        "4. Operational Data Generation:\n",
        "* The process is repeated for the number of iterations specified.\n",
        "* For each device in the device_ids list, the script generates one record of operational data. This data includes:\n",
        "  * The device_id.\n",
        "  * A timestamp generated using the Faker library to simulate a datetime within the last 30 days.\n",
        "  * A battery_level, which is a random integer from 0 to 100%.\n",
        "  * The firmware_version, generated by the random_firmware_version function.\n",
        "  * The connectivity_status, generated by the random_connectivity_status function.\n",
        "  * Any error_codes, generated by the random_error_codes function.\n",
        "\n",
        "5. Writing to JSON File:\n",
        "\n",
        "* The generated data for each iteration is added to the operational_data list.\n",
        "* After each iteration, the data is written to a JSON file, named uniquely using a timestamp.\n",
        "* A message is printed to indicate the successful generation and saving of the operational data records.\n",
        "* The operational_data list is cleared at the end of each iteration to prepare for the next set of data generation.\n"
      ],
      "metadata": {
        "id": "4jp5Q3FJrmZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mysql.connector\n",
        "\n",
        "# MySQL database credentials\n",
        "host= DB_HOST\n",
        "port= DB_PORT\n",
        "username= DB_USERNAME\n",
        "password= DB_PASSWORD\n",
        "database= DB_NAME\n",
        "\n",
        "\n",
        "# Establish a connection to the MySQL database\n",
        "cnx = mysql.connector.connect(user=username, password=password,\n",
        "                              host=host, database=database)\n",
        "\n",
        "cursor = cnx.cursor()\n",
        "sql_str='SELECT COUNT(*) FROM Patient;'\n",
        "rs=cursor.execute(sql_str)\n",
        "rs=cursor.fetchall()\n",
        "print(rs)\n",
        "\n",
        "# Fetch existing device IDs\n",
        "cursor.execute(\"SELECT device_id FROM DeviceStaticInfo\")\n",
        "device_ids = [row[0] for row in cursor.fetchall()]\n",
        "print(device_ids)\n",
        "\n",
        "cursor.execute(\"SELECT patient_id FROM Patient\")\n",
        "patient_ids = [row[0] for row in cursor.fetchall()]\n",
        "print(patient_ids)"
      ],
      "metadata": {
        "id": "OSA7Ip_lX816",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffa6d305-08a2-4e97-9edf-a9a3e011efe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(20,)]\n",
            "[1, 2, 5, 3, 4]\n",
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Glucose Data\n",
        "\n",
        "1. Defining User Profiles: Five user profiles are created - 'diabetic', 'athlete', 'party-goer', 'low-glucose', and 'elderly'. These represent different types of individuals who might have varying glucose levels.\n",
        "\n",
        "2. Mapping Patient IDs to User Profiles: Each patient ID from the provided patient_ids list is randomly assigned one of the user profiles. This establishes a variety of simulated patients.\n",
        "\n",
        "3. TimeSynth Setup: TimeSynth, a library used for generating synthetic time series data, is set up here. It samples a regular time point and creates a sinusoidal signal with added Gaussian noise. This setup is used to simulate the natural fluctuations in glucose levels over time.\n",
        "\n",
        "4. Glucose Reading Generation: For each profile, a function generate_glucose_reading is defined, which generates glucose readings based on the profile's characteristics:\n",
        "  * Diabetic: Higher readings.\n",
        "  * Athlete: Lower post-exercise readings.\n",
        "  * Party-goer: Variable readings.\n",
        "  * Low-glucose: Dangerously low readings.\n",
        "  * Elderly: Steady/higher readings.\n",
        "\n",
        "The readings are modified based on the sinusoidal signal and noise, simulating natural variations.\n",
        "\n",
        "5. Generating Readings:\n",
        "\n",
        "  * Iterations: The code runs for the number of iterations specified.\n",
        "  * For each iteration and each patient, the corresponding profile is identified, and glucose readings are generated based on that profile.\n",
        "  * Each reading includes the patient's ID, a randomly chosen device ID from device_ids, the glucose level (rounded to 2 decimal places), a timestamp (generated using the Faker library to simulate a datetime within the last 30 days), and a location (also generated by Faker).\n",
        "\n",
        "6. Writing to JSON:\n",
        "  * After generating readings for each patient in an iteration, the data is written to a JSON file.\n",
        "  * The file is named with a timestamp to ensure uniqueness.\n",
        "  * After writing the data, the readings list is reset for the next iteration."
      ],
      "metadata": {
        "id": "cQXzmU4Irq4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_device_readings(iterations, device_ids):\n",
        "  # Function to generate random firmware version\n",
        "  def random_firmware_version():\n",
        "      major = random.randint(1, 3)\n",
        "      minor = random.randint(0, 9)\n",
        "      patch = random.randint(0, 9)\n",
        "      return f\"{major}.{minor}.{patch}\"\n",
        "\n",
        "  # Function to generate random connectivity status\n",
        "  def random_connectivity_status():\n",
        "      return random.choice(['Connected', 'Disconnected', 'Poor Connection'])\n",
        "\n",
        "  # Function to generate random error codes\n",
        "  def random_error_codes():\n",
        "      if random.choice([True, False]):\n",
        "          return random.choice(['Err1', 'Err2', 'Err3', ''])\n",
        "      return ''\n",
        "\n",
        "  # List to store generated operational data\n",
        "  operational_data = []\n",
        "\n",
        "  # Generate and add device operational data to the list\n",
        "  for i in range(iterations):\n",
        "    for device_id in device_ids:\n",
        "        for _ in range(1):  # Generate 1 record per device\n",
        "            data = {\n",
        "                'device_id': device_id,\n",
        "                'timestamp': fake.date_time_between(start_date=\"-30d\", end_date=\"now\").strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                'battery_level': random.randint(0, 100),  # Battery level from 0 to 100%\n",
        "                'firmware_version': random_firmware_version(),\n",
        "                'connectivity_status': random_connectivity_status(),\n",
        "                'error_codes': random_error_codes()\n",
        "            }\n",
        "            operational_data.append(data)\n",
        "\n",
        "    # Write the generated operational data to a JSON file\n",
        "    with open(f'./device_readings_dir/device_operational_data_{datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S_%f\")}.json', 'w') as file:\n",
        "        json.dump(operational_data, file, indent=4)\n",
        "        print(f\"Generated {len(operational_data)} device operational data records and saved them to '{file.name}'\")\n",
        "\n",
        "    #clear\n",
        "    operational_data=[]"
      ],
      "metadata": {
        "id": "7RO-GcEdiYSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/TimeSynth/TimeSynth.git\n",
        "%cd TimeSynth\n",
        "!pip install .\n",
        "!pip install timesynth\n",
        "%cd ..\n",
        "#Then restart runtime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMWsotk904cV",
        "outputId": "166d2a33-f4b9-46e3-f4e3-83468159406a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'TimeSynth' already exists and is not an empty directory.\n",
            "/content/TimeSynth\n",
            "Processing /content/TimeSynth\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from timesynth==0.2.4) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from timesynth==0.2.4) (1.11.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from timesynth==0.2.4) (1.12)\n",
            "Requirement already satisfied: symengine>=0.4 in /usr/local/lib/python3.10/dist-packages (from timesynth==0.2.4) (0.11.0)\n",
            "Requirement already satisfied: jitcdde==1.4 in /usr/local/lib/python3.10/dist-packages (from timesynth==0.2.4) (1.4.0)\n",
            "Requirement already satisfied: jitcxde_common==1.4.1 in /usr/local/lib/python3.10/dist-packages (from timesynth==0.2.4) (1.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from jitcxde_common==1.4.1->timesynth==0.2.4) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from jitcxde_common==1.4.1->timesynth==0.2.4) (67.7.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->timesynth==0.2.4) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->jitcxde_common==1.4.1->timesynth==0.2.4) (2.1.3)\n",
            "Building wheels for collected packages: timesynth\n",
            "  Building wheel for timesynth (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for timesynth: filename=timesynth-0.2.4-py3-none-any.whl size=15422 sha256=12b55bf7ef3132bee314233acee5e48afa559bb510b5033f6b8ff89ca777cb8d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dfk3s13e/wheels/81/84/c6/880288cb791ed65eb7343a49a4918038d6e3bf7622d3d187e0\n",
            "Successfully built timesynth\n",
            "Installing collected packages: timesynth\n",
            "  Attempting uninstall: timesynth\n",
            "    Found existing installation: timesynth 0.2.4\n",
            "    Uninstalling timesynth-0.2.4:\n",
            "      Successfully uninstalled timesynth-0.2.4\n",
            "Successfully installed timesynth-0.2.4\n",
            "Requirement already satisfied: timesynth in /usr/local/lib/python3.10/dist-packages (0.2.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from timesynth) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from timesynth) (1.11.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from timesynth) (1.12)\n",
            "Requirement already satisfied: symengine>=0.4 in /usr/local/lib/python3.10/dist-packages (from timesynth) (0.11.0)\n",
            "Requirement already satisfied: jitcdde==1.4 in /usr/local/lib/python3.10/dist-packages (from timesynth) (1.4.0)\n",
            "Requirement already satisfied: jitcxde-common==1.4.1 in /usr/local/lib/python3.10/dist-packages (from timesynth) (1.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from jitcxde-common==1.4.1->timesynth) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from jitcxde-common==1.4.1->timesynth) (67.7.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->timesynth) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->jitcxde-common==1.4.1->timesynth) (2.1.3)\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import timesynth as ts\n",
        "import numpy as np\n",
        "import random\n",
        "from faker import Faker\n",
        "\n",
        "\n",
        "def generate_glucose_readings(iterations, patient_ids, device_ids):\n",
        "    # Define user profiles\n",
        "    user_profiles = ['diabetic', 'athlete', 'party-goer', 'low-glucose', 'elderly']\n",
        "\n",
        "    # Map patient IDs to user profiles\n",
        "    patient_profile_map = {patient_id: random.choice(user_profiles) for patient_id in patient_ids}\n",
        "\n",
        "    # TimeSynth setup\n",
        "    time_sampler = ts.TimeSampler(stop_time=1)\n",
        "    regular_time_samples = time_sampler.sample_regular_time(num_points=1)\n",
        "    sinusoid = ts.signals.Sinusoidal(frequency=0.25)\n",
        "    white_noise = ts.noise.GaussianNoise(std=0.3)\n",
        "    timeseries = ts.TimeSeries(sinusoid, noise_generator=white_noise)\n",
        "\n",
        "    # Glucose reading generator based on user profile\n",
        "    def generate_glucose_reading(profile):\n",
        "      samples, _, _ = timeseries.sample(regular_time_samples)\n",
        "\n",
        "      if profile == 'diabetic':\n",
        "          return samples * 40 + 130  # Higher readings\n",
        "      elif profile == 'athlete':\n",
        "          return samples * 20 + 90   # Lower post-exercise readings\n",
        "      elif profile == 'party-goer':\n",
        "          return samples * 50 + 130  # Variable readings\n",
        "      elif profile == 'low-glucose':\n",
        "          return samples * 15 + 55   # Dangerously low readings\n",
        "      elif profile == 'elderly':\n",
        "          return samples * 35 + 145  # Steady/higher readings\n",
        "      else:\n",
        "          return samples * 40 + 110  # Normal range for other patients\n",
        "\n",
        "\n",
        "    # List to store generated readings\n",
        "    readings = []\n",
        "\n",
        "    # Generate glucose readings\n",
        "    for i in range(iterations):\n",
        "        for patient_id in patient_ids:\n",
        "            profile = patient_profile_map[patient_id]\n",
        "            glucose_levels = generate_glucose_reading(profile)\n",
        "\n",
        "            for glucose_level in glucose_levels:\n",
        "\n",
        "                reading = {\n",
        "                    'patient_id': patient_id,\n",
        "                    'device_id': random.choice(device_ids),\n",
        "                    'glucose_level': round(glucose_level, 2),\n",
        "                    'timestamp': fake.date_time_between(start_date=\"-30d\", end_date=\"now\").strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                    'location': fake.city()\n",
        "                }\n",
        "                readings.append(reading)\n",
        "\n",
        "        # Write the generated readings to a JSON file\n",
        "        with open(f'./glucose_readings_dir/glucose_readings_corrected_{datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S_%f\")}.json', 'w') as file:\n",
        "            json.dump(readings, file, indent=4)\n",
        "            print(f\"Generated {iterations} glucose reading(s) for each patient and saved them to '{file.name}'\")\n",
        "\n",
        "        readings = []\n"
      ],
      "metadata": {
        "id": "m-hq2S81-IFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate Device Data: 1 reading per device — 1 json file will be generated with all readings\n",
        "#by default we have 5 devices\n",
        "generate_device_readings(1, device_ids)\n",
        "\n",
        "#Generate Glucse Readings: 1 sample for each patient — 1 json file will be generated with all readings\n",
        "#by default we have 20 patients\n",
        "generate_glucose_readings(15, patient_ids, device_ids )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncLdmoUj04qe",
        "outputId": "0917d637-1dd8-4012-826d-499712c9b2d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 5 device operational data records and saved them to './device_readings_dir/device_operational_data_2024_01_02_22_16_23_182121.json'\n",
            "Generated 15 glucose reading(s) for each patient and saved them to './glucose_readings_dir/glucose_readings_corrected_2024_01_02_22_16_23_202632.json'\n",
            "Generated 15 glucose reading(s) for each patient and saved them to './glucose_readings_dir/glucose_readings_corrected_2024_01_02_22_16_23_212910.json'\n",
            "Generated 15 glucose reading(s) for each patient and saved them to './glucose_readings_dir/glucose_readings_corrected_2024_01_02_22_16_23_221518.json'\n",
            "Generated 15 glucose reading(s) for each patient and saved them to './glucose_readings_dir/glucose_readings_corrected_2024_01_02_22_16_23_230911.json'\n",
            "Generated 15 glucose reading(s) for each patient and saved them to './glucose_readings_dir/glucose_readings_corrected_2024_01_02_22_16_23_240336.json'\n",
            "Generated 15 glucose reading(s) for each patient and saved them to './glucose_readings_dir/glucose_readings_corrected_2024_01_02_22_16_23_251549.json'\n",
            "Generated 15 glucose reading(s) for each patient and saved them to './glucose_readings_dir/glucose_readings_corrected_2024_01_02_22_16_23_262628.json'\n",
            "Generated 15 glucose reading(s) for each patient and saved them to './glucose_readings_dir/glucose_readings_corrected_2024_01_02_22_16_23_270082.json'\n",
            "Generated 15 glucose reading(s) for each patient and saved them to './glucose_readings_dir/glucose_readings_corrected_2024_01_02_22_16_23_278475.json'\n",
            "Generated 15 glucose reading(s) for each patient and saved them to './glucose_readings_dir/glucose_readings_corrected_2024_01_02_22_16_23_287999.json'\n",
            "Generated 15 glucose reading(s) for each patient and saved them to './glucose_readings_dir/glucose_readings_corrected_2024_01_02_22_16_23_299057.json'\n",
            "Generated 15 glucose reading(s) for each patient and saved them to './glucose_readings_dir/glucose_readings_corrected_2024_01_02_22_16_23_311507.json'\n",
            "Generated 15 glucose reading(s) for each patient and saved them to './glucose_readings_dir/glucose_readings_corrected_2024_01_02_22_16_23_319939.json'\n",
            "Generated 15 glucose reading(s) for each patient and saved them to './glucose_readings_dir/glucose_readings_corrected_2024_01_02_22_16_23_331029.json'\n",
            "Generated 15 glucose reading(s) for each patient and saved them to './glucose_readings_dir/glucose_readings_corrected_2024_01_02_22_16_23_340720.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to generate data in the background:"
      ],
      "metadata": {
        "id": "5hIDErt59TBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a process to run the generate_data function in the background\n",
        "device_readings_process = multiprocessing.Process(target=generate_device_readings, args=(1, device_ids))\n",
        "glucose_readings_process = multiprocessing.Process(target=generate_glucose_readings, args=(1, patient_ids, device_ids))\n",
        "\n",
        "# Start the process\n",
        "device_readings_process.start()\n",
        "glucose_readings_process.start()"
      ],
      "metadata": {
        "id": "IzrAJ4KfcHNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show Generated Data"
      ],
      "metadata": {
        "id": "dl9NALqAAKOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Opening JSON file\n",
        "f = open('glucose_readings_dir/glucose_readings_corrected_2024_01_02_11_42_06_426773.json')\n",
        "\n",
        "# returns JSON object as\n",
        "# a dictionary\n",
        "data = json.load(f)\n",
        "\n",
        "# Iterating through the json\n",
        "# list\n",
        "for i in data:\n",
        "    print(i)\n",
        "\n",
        "# Closing file\n",
        "f.close()"
      ],
      "metadata": {
        "id": "S8nj8PXflFeu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12092fbd-4837-46c7-b882-84f8c01f656e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'patient_id': 1, 'device_id': 3, 'glucose_level': 107.58, 'timestamp': '2023-12-23 04:04:09', 'location': 'South John'}\n",
            "{'patient_id': 2, 'device_id': 2, 'glucose_level': 119.24, 'timestamp': '2023-12-22 16:22:07', 'location': 'Reneeland'}\n",
            "{'patient_id': 3, 'device_id': 2, 'glucose_level': 128.09, 'timestamp': '2023-12-08 11:26:49', 'location': 'Morgantown'}\n",
            "{'patient_id': 4, 'device_id': 4, 'glucose_level': 147.17, 'timestamp': '2023-12-12 23:09:57', 'location': 'North Martinstad'}\n",
            "{'patient_id': 5, 'device_id': 4, 'glucose_level': 135.7, 'timestamp': '2023-12-26 21:58:35', 'location': 'North Vincentmouth'}\n",
            "{'patient_id': 6, 'device_id': 4, 'glucose_level': 153.98, 'timestamp': '2023-12-10 08:22:53', 'location': 'Christophershire'}\n",
            "{'patient_id': 7, 'device_id': 1, 'glucose_level': 134.78, 'timestamp': '2023-12-19 12:37:24', 'location': 'West Sarah'}\n",
            "{'patient_id': 8, 'device_id': 5, 'glucose_level': 126.55, 'timestamp': '2023-12-11 20:08:11', 'location': 'Shannonland'}\n",
            "{'patient_id': 9, 'device_id': 1, 'glucose_level': 109.62, 'timestamp': '2023-12-28 07:51:25', 'location': 'Vegashire'}\n",
            "{'patient_id': 10, 'device_id': 4, 'glucose_level': 77.62, 'timestamp': '2023-12-12 07:01:55', 'location': 'East Alicia'}\n",
            "{'patient_id': 11, 'device_id': 1, 'glucose_level': 126.56, 'timestamp': '2023-12-18 19:18:52', 'location': 'Connerfurt'}\n",
            "{'patient_id': 12, 'device_id': 3, 'glucose_level': 155.93, 'timestamp': '2023-12-31 20:11:21', 'location': 'Michelletown'}\n",
            "{'patient_id': 13, 'device_id': 5, 'glucose_level': 159.19, 'timestamp': '2023-12-16 16:58:47', 'location': 'Alexistown'}\n",
            "{'patient_id': 14, 'device_id': 5, 'glucose_level': 135.56, 'timestamp': '2023-12-29 16:13:02', 'location': 'South Diane'}\n",
            "{'patient_id': 15, 'device_id': 2, 'glucose_level': 142.38, 'timestamp': '2023-12-29 01:28:17', 'location': 'Lake Sara'}\n",
            "{'patient_id': 16, 'device_id': 1, 'glucose_level': 82.54, 'timestamp': '2023-12-28 09:01:11', 'location': 'Katherinehaven'}\n",
            "{'patient_id': 17, 'device_id': 1, 'glucose_level': 156.68, 'timestamp': '2023-12-08 01:43:56', 'location': 'Christineberg'}\n",
            "{'patient_id': 18, 'device_id': 1, 'glucose_level': 96.63, 'timestamp': '2023-12-21 18:46:00', 'location': 'Michelleside'}\n",
            "{'patient_id': 19, 'device_id': 2, 'glucose_level': 134.4, 'timestamp': '2023-12-31 18:29:19', 'location': 'Torresbury'}\n",
            "{'patient_id': 20, 'device_id': 1, 'glucose_level': 143.02, 'timestamp': '2023-12-06 10:43:01', 'location': 'Lake Markland'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Opening JSON file\n",
        "f = open('/content/device_readings_dir/device_operational_data_2024_01_02_11_46_16_130197.json')\n",
        "\n",
        "# returns JSON object as\n",
        "# a dictionary\n",
        "data = json.load(f)\n",
        "\n",
        "# Iterating through the json\n",
        "# list\n",
        "for i in data:\n",
        "    print(i)\n",
        "\n",
        "# Closing file\n",
        "f.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIneAZRkAQpg",
        "outputId": "313383f0-30db-457f-a6f4-6b2342984373"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'device_id': 1, 'timestamp': '2023-12-30 08:09:24', 'battery_level': 57, 'firmware_version': '1.3.7', 'connectivity_status': 'Disconnected', 'error_codes': ''}\n",
            "{'device_id': 2, 'timestamp': '2023-12-17 18:18:04', 'battery_level': 55, 'firmware_version': '2.7.2', 'connectivity_status': 'Poor Connection', 'error_codes': ''}\n",
            "{'device_id': 5, 'timestamp': '2023-12-13 02:24:01', 'battery_level': 15, 'firmware_version': '2.9.0', 'connectivity_status': 'Disconnected', 'error_codes': ''}\n",
            "{'device_id': 3, 'timestamp': '2024-01-02 05:18:39', 'battery_level': 68, 'firmware_version': '3.8.3', 'connectivity_status': 'Poor Connection', 'error_codes': ''}\n",
            "{'device_id': 4, 'timestamp': '2023-12-08 20:35:51', 'battery_level': 79, 'firmware_version': '1.6.1', 'connectivity_status': 'Disconnected', 'error_codes': ''}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Send Data to Eventhub\n",
        "1. Define Schema\n",
        "2. Connect to Eventhub\n",
        "3. Send data in constant manner by checking the folder with generated data (JSONs)"
      ],
      "metadata": {
        "id": "t1gHLO60AZG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Process**\n",
        "1. Connection String: The variable connection_string is set to store the connection string for the Azure Event Hub. This connection string is required to authenticate and establish a connection with your Event Hub instance.\n",
        "\n",
        "2. Function send_to_eventhub_batch:\n",
        "\n",
        "* The function takes two arguments: batch_df, which is a Spark DataFrame representing a batch of data, and batch_id, which identifies the batch (though batch_id isn't explicitly used in the function).\n",
        "* First, the function checks if the DataFrame batch_df is not empty.\n",
        "* It then creates an EventHubProducerClient instance using the connection string and the name of the Event Hub (YOUR_EVENTHUB_NAME).\n",
        "* Within a with statement (which ensures proper resource management), a batch of events is created using producer.create_batch().\n",
        "\n",
        "* The function iterates over each row in the DataFrame. For each row, it:\n",
        "  * Converts the row to a dictionary and then to a string.\n",
        "  * Creates an EventData object from this string.\n",
        "  * Tries to add the EventData object to the current event data batch.\n",
        "  * If the batch is full (indicated by a ValueError), the batch is sent using producer.send_batch(), and a new batch is created to add the event.\n",
        "* After all rows are processed, any remaining events in the current batch are sent to Event Hubs.\n",
        "\n",
        "3. Streaming Query: The code treats a static DataFrame (json_df_glucose) as a stream.\n",
        "* It defines a streaming query using the writeStream method. The query uses the foreachBatch method, which applies the send_to_eventhub_batch function to each batch of data in the stream.\n",
        "* The start() method starts the streaming query, and awaitTermination() keeps the query running until it's either stopped manually or an error occurs.\n"
      ],
      "metadata": {
        "id": "TNTAZrfkrvAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install azure.eventhub"
      ],
      "metadata": {
        "id": "0RB2dII0ApQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"JSONtoEventHub\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# schema definitions\n",
        "glucose_schema = StructType([\n",
        "    StructField(\"patient_id\", IntegerType(), True),\n",
        "    StructField(\"device_id\", IntegerType(), True),\n",
        "    StructField(\"glucose_level\", FloatType(), True),\n",
        "    StructField(\"timestamp\", TimestampType(), True),\n",
        "    StructField(\"location\", StringType(), True)\n",
        "])\n",
        "\n",
        "device_schema = StructType([\n",
        "    StructField(\"device_id\", IntegerType(), True),\n",
        "    StructField(\"timestamp\", TimestampType(), True),\n",
        "    StructField(\"battery_level\", FloatType(), True),\n",
        "    StructField(\"firmware_version\", StringType(), True),\n",
        "    StructField(\"connectivity_status\", StringType(), True),\n",
        "    StructField(\"error_codes\", StringType(), True)\n",
        "])\n",
        "\n",
        "\n",
        "# Read JSON file into DataFrame\n",
        "json_df_glucose = spark.readStream.option('multiline', True).schema(glucose_schema).json(\"./glucose_readings_dir/\")\n",
        "json_df_device = spark.readStream.option('multiline', True).schema(device_schema).json(\"./device_readings_dir/\")\n",
        "#.option('multiline', True)"
      ],
      "metadata": {
        "id": "G4Nm3-JeApQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.eventhub import EventHubProducerClient, EventData\n",
        "\n",
        "connection_string = YOUR_CONNECTION_STRING\n",
        "\n",
        "def send_to_eventhub_batch(batch_df, batch_id):\n",
        "    if not batch_df.rdd.isEmpty():\n",
        "        producer = EventHubProducerClient.from_connection_string(\n",
        "            conn_str=connection_string,\n",
        "            eventhub_name=\"glucose_monitoring\"\n",
        "        )\n",
        "        with producer:\n",
        "            event_data_batch = producer.create_batch()\n",
        "            for row in batch_df.collect():\n",
        "                event_data = EventData(str(row.asDict()))\n",
        "                try:\n",
        "                    # Add the event to the batch\n",
        "                    event_data_batch.add(event_data)\n",
        "                except ValueError:\n",
        "                    # The batch is full, send it and start a new batch\n",
        "                    producer.send_batch(event_data_batch)\n",
        "                    event_data_batch = producer.create_batch()\n",
        "                    event_data_batch.add(event_data)  # Add the event to the new batch\n",
        "            # Send any remaining events in the batch\n",
        "            if len(event_data_batch) > 0:\n",
        "                producer.send_batch(event_data_batch)\n",
        "\n",
        "\n",
        "# Streaming query (treating the static DataFrame as a stream)\n",
        "query = json_df_glucose.writeStream \\\n",
        "    .foreachBatch(send_to_eventhub_batch) \\\n",
        "    .start()\\\n",
        "    .awaitTermination()"
      ],
      "metadata": {
        "id": "6FG4PNNjApQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.eventhub import EventHubProducerClient, EventData\n",
        "\n",
        "connection_string = YOUR_CONNECTION_STRING\n",
        "\n",
        "def send_to_eventhub_batch(batch_df, batch_id):\n",
        "    if not batch_df.rdd.isEmpty():\n",
        "        producer = EventHubProducerClient.from_connection_string(\n",
        "            conn_str=connection_string,\n",
        "            eventhub_name=\"device_readings\"\n",
        "        )\n",
        "        with producer:\n",
        "            event_data_batch = producer.create_batch()\n",
        "            for row in batch_df.collect():\n",
        "                event_data = EventData(str(row.asDict()))\n",
        "                try:\n",
        "                    # Add the event to the batch\n",
        "                    event_data_batch.add(event_data)\n",
        "                except ValueError:\n",
        "                    # The batch is full, send it and start a new batch\n",
        "                    producer.send_batch(event_data_batch)\n",
        "                    event_data_batch = producer.create_batch()\n",
        "                    event_data_batch.add(event_data)  # Add the event to the new batch\n",
        "            # Send any remaining events in the batch\n",
        "            if len(event_data_batch) > 0:\n",
        "                producer.send_batch(event_data_batch)\n",
        "\n",
        "\n",
        "# Streaming query (treating the static DataFrame as a stream)\n",
        "query = json_df_device.writeStream \\\n",
        "    .foreachBatch(send_to_eventhub_batch) \\\n",
        "    .start()\\\n",
        "    .awaitTermination()"
      ],
      "metadata": {
        "id": "ctgI5x5VCFEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) Filter Data for Alerts\n",
        "1. Condition: (Glucose Reading > 115) OR (Rolling Window Average of 10 minutes > 105)\n",
        "2. Send filtered data to Eventhub\n",
        "3. From Eventhub, Azure Functions picks data and send alert to Telegram channel.\n",
        "\n",
        "\n",
        "How Code works:\n",
        "1. Read the streaming data from JSON files.\n",
        "2. Use window functions to calculate the rolling average. Since the files are in chronological order, make sure the files are ingested by the streaming job in the same order.\n",
        "3. Apply the filter based on the rolling average and the individual glucose level readings.\n",
        "\n",
        "\n",
        "This code will calculate the rolling average for the last 10 minutes of glucose readings for each patient, and it assumes that each file is treated as a separate micro-batch in chronological order. If data is not coming all the time, the average will be calculated over however many readings are available.\n",
        "\n"
      ],
      "metadata": {
        "id": "eCj82oe4GID6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg, window\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType\n",
        "from azure.eventhub import EventHubProducerClient, EventData\n",
        "\n",
        "# Initialize Spark Session for Streaming\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"GlucoseStreamingAnalysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Define schema for incoming JSON data\n",
        "glucose_schema = StructType([\n",
        "    StructField(\"patient_id\", IntegerType(), True),\n",
        "    StructField(\"device_id\", IntegerType(), True),\n",
        "    StructField(\"glucose_level\", FloatType(), True),\n",
        "    StructField(\"timestamp\", TimestampType(), True),\n",
        "    StructField(\"location\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Read streaming data from JSON files\n",
        "json_df_glucose = spark.readStream \\\n",
        "    .schema(glucose_schema) \\\n",
        "    .json(\"./glucose_readings_dir/\")\n",
        "\n",
        "# Define window (e.g., \"10 minutes\")\n",
        "window_duration = \"10 minutes\"\n",
        "\n",
        "# Group data by patient_id and window, then calculate rolling average\n",
        "rolling_avg_df = json_df_glucose.groupBy(\"patient_id\", window(\"timestamp\", window_duration)) \\\n",
        "    .agg(avg(\"glucose_level\").alias(\"rolling_avg\"))\n",
        "\n",
        "# Filter based on conditions\n",
        "condition1 = col(\"glucose_level\") > 115\n",
        "condition2 = col(\"rolling_avg\") > 105\n",
        "\n",
        "high_glucose_df = json_df_glucose.filter(condition1)\n",
        "rolling_avg_high_df = rolling_avg_df.filter(condition2).select(\"patient_id\", \"window\", \"rolling_avg\")\n",
        "\n",
        "# Combine results\n",
        "final_df = high_glucose_df.select(\"patient_id\", \"glucose_level\", \"timestamp\") \\\n",
        "    .union(rolling_avg_high_df)\n",
        "\n",
        "connection_string = 'YOUR_CONNECTION_STRING'\n",
        "\n",
        "\n",
        "\n",
        "def send_to_eventhub_batch(batch_df, batch_id):\n",
        "    if not batch_df.rdd.isEmpty():\n",
        "        producer = EventHubProducerClient.from_connection_string(\n",
        "            conn_str=connection_string,\n",
        "            eventhub_name=\"alerts\"\n",
        "        )\n",
        "        with producer:\n",
        "            event_data_batch = producer.create_batch()\n",
        "            for row in batch_df.collect():\n",
        "                event_data = EventData(str(row.asDict()))\n",
        "                try:\n",
        "                    # Add the event to the batch\n",
        "                    event_data_batch.add(event_data)\n",
        "                except ValueError:\n",
        "                    # The batch is full, send it and start a new batch\n",
        "                    producer.send_batch(event_data_batch)\n",
        "                    event_data_batch = producer.create_batch()\n",
        "                    event_data_batch.add(event_data)  # Add the event to the new batch\n",
        "            # Send any remaining events in the batch\n",
        "            if len(event_data_batch) > 0:\n",
        "                producer.send_batch(event_data_batch)\n",
        "\n",
        "# Streaming query (treating the static DataFrame as a stream)\n",
        "query = final_df.writeStream \\\n",
        "    .foreachBatch(send_to_eventhub_batch) \\\n",
        "    .outputMode(\"update\") \\\n",
        "    .start() \\\n",
        "    .awaitTermination()"
      ],
      "metadata": {
        "id": "lPNcvPcUCFMC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}