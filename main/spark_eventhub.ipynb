{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Receive Data from Eventhub, Filter Alerts, and Send Back"
      ],
      "metadata": {
        "id": "TDvzUOioXRbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark_release='spark-3.4.2'\n",
        "hadoop_version='hadoop3'\n",
        "\n",
        "import os, time\n",
        "start=time.time()\n",
        "os.environ['SPARK_RELEASE']=spark_release\n",
        "os.environ['HADOOP_VERSION']=hadoop_version\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_release}-bin-{hadoop_version}\""
      ],
      "metadata": {
        "id": "L4Qmn3xmXRC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run below commands in google colab\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # install Java8\n",
        "!wget -q http://apache.osuosl.org/spark/${SPARK_RELEASE}/${SPARK_RELEASE}-bin-${HADOOP_VERSION}.tgz # download spark-3.3.X\n",
        "!tar xf ${SPARK_RELEASE}-bin-${HADOOP_VERSION}.tgz # unzip it\n",
        "!pip install -q findspark # install findspark"
      ],
      "metadata": {
        "id": "polFdAYw-22g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker pysqlite3\n",
        "!pip install mysql.connector\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "-2DQkwDEbV-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87bd6329-c92a-4e07-916e-f0246be7a02d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mysql.connector in /usr/local/lib/python3.10/dist-packages (2.2.9)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark"
      ],
      "metadata": {
        "id": "bnJ-4Q8NyMNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run below commands in google colab\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # install Java8\n",
        "!wget -q http://apache.osuosl.org/spark/${SPARK_RELEASE}/${SPARK_RELEASE}-bin-${HADOOP_VERSION}.tgz # download spark-3.3.X\n",
        "!tar xf ${SPARK_RELEASE}-bin-${HADOOP_VERSION}.tgz # unzip it\n",
        "!pip install -q findspark # install findspark"
      ],
      "metadata": {
        "id": "wl8IziHQycfP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "214f8ef2-10a0-4955-9d5f-a3caabc08021"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tar: spark-3.3.3-bin-hadoop3.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "import multiprocessing\n",
        "import pyspark\n",
        "import socket\n",
        "import uuid\n",
        "import findspark\n",
        "from pyspark.conf import SparkConf\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import expr\n",
        "from pyspark.sql.streaming import DataStreamReader\n",
        "\n",
        "import sqlite3\n",
        "from faker import Faker\n",
        "import random\n",
        "import datetime\n",
        "import json\n",
        "fake=Faker()"
      ],
      "metadata": {
        "id": "0ednyWm-bt52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# findspark find your Spark Distribution and sets necessary environment variables\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "tc2xPTOJyee0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the pyspark version\n",
        "print(pyspark.__version__)"
      ],
      "metadata": {
        "id": "7h-gacbeygMT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "341c9268-d6e9-441a-864e-f4e607f7d9e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DStream that will connect to hostname:port, like localhost:9999\n",
        "# if doing this over a network, firewalls may block the connection!\n",
        "hostname=socket.gethostname()\n",
        "\n",
        "hostname"
      ],
      "metadata": {
        "id": "N1G55x3fymTE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a19d23be-6abd-4842-ce4b-3db90d507fc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'15d1e066aaaa'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "app_id=str(uuid.uuid1())\n",
        "\n",
        "app_id"
      ],
      "metadata": {
        "id": "qjL2UCxoyoBy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b218fb78-0912-4fac-88a5-1b64838a081a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'89ab8e7c-a95d-11ee-91eb-0242ac1c000c'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conf = SparkConf()\n",
        "\n",
        "conf.setAll([\n",
        "     ('spark.app.name', app_id),\n",
        "     ('spark.shuffle.useOldFetchProtocol', 'true'),\n",
        "     ('spark.testing', 'true'), # Avoid minimum 450M executor/driver memory https://www.waitingforcode.com/apache-spark/troubleshooting-system-memory-must-be-at-least-error/read / https://programmerclick.com/article/72821685476/\n",
        "     ('spark.driver.allowMultipleContexts','true'), # https://stackoverflow.com/a/41591258 This option is used only for Spark internal tests and is not to be used in production.\n",
        "     ('spark. y', '100M'),\n",
        "     # ('spark.driver.memory ', '200M'),\n",
        "     # ('spark.executor.instances',1), # This property is no longer used in Spark 2+\n",
        "     # number of executors is determined as: floor(spark.cores.max / spark.executor.cores)\n",
        "     (\"spark.executor.cores\",1), # cores per executor. https://stackoverflow.com/questions/39399205/spark-standalone-number-executors-cores-control/39400195#39400195\n",
        "     (\"spark.cores.max\", 2), # the maximum amount of CPU cores to request for the application from across the cluster (not from each machine)\n",
        "     ('spark.submit.deployMode', 'client'), # client, cluster\n",
        "     ('spark.ui.showConsoleProgress', 'true'),\n",
        "     (\"fs.azure\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\") ])"
      ],
      "metadata": {
        "id": "8LlJ3BHFyogz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a79c954-c466-4c4b-e4dd-d6c5b33ff96d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.conf.SparkConf at 0x7a3103a38be0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "end=time.time()\n",
        "\n",
        "\n",
        "f'Spark setup time: {int(end-start)} seconds'"
      ],
      "metadata": {
        "id": "_J85b_QsysNI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "595f14ba-6252-4d3f-a891-c1839042ea27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Spark setup time: 110 seconds'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Data from Eventhub\n",
        "1. Set-up PySpark environment for Kafka/Eventhub integration\n",
        "2. Set-up configuration for Kafka-Eventhub Connection\n",
        "3. Reading from Event Hub\n",
        "4. Querying the Streaming Data\n",
        "\n"
      ],
      "metadata": {
        "id": "NZN5DFVdPLcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg, count\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, TimestampType, StringType\n",
        "from azure.eventhub import EventHubProducerClient, EventData\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"StreamingFromEventHub\") \\\n",
        "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0') \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Kafka Configuration for reading from Event Hub\n",
        "kafkaConf = {\n",
        "    \"kafka.bootstrap.servers\": \"eventhubname.servicebus.windows.net:9093\",\n",
        "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
        "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
        "    \"kafka.sasl.jaas.config\": 'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"Endpoint=sb://your_address.servicebus.windows.net/;SharedAccessKeyName=YOUR_ACCESS_KEY;SharedAccessKey=YOUR_ACCESS_KEY;EntityPath=TOPIC\";',\n",
        "    \"subscribe\": \"TOPIC\",\n",
        "    \"group.id\": \"CONSUMER_GROUP\",\n",
        "    \"startingOffsets\": \"earliest\"\n",
        "}\n",
        "\n",
        "# Schema definition for glucose readings\n",
        "glucose_schema = StructType([\n",
        "    StructField(\"patient_id\", IntegerType(), True),\n",
        "    StructField(\"device_id\", IntegerType(), True),\n",
        "    StructField(\"glucose_level\", FloatType(), True),\n",
        "    StructField(\"timestamp\", TimestampType(), True),\n",
        "    StructField(\"location\", StringType(), True)\n",
        "])\n",
        "\n",
        "\n",
        " # Read from Event Hub using Kafka\n",
        "df = spark \\\n",
        "    .readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .options(**kafkaConf) \\\n",
        "    .load() \\\n",
        "    .selectExpr(\"CAST(value AS STRING) as json\") \\\n",
        "    .select(from_json(\"json\", glucose_schema).alias(\"data\")) \\\n",
        "    .select(\"data.*\")\n",
        "\n",
        "# Output to console\n",
        "query = df.writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"table\") \\\n",
        "    .start()\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "BJEJZxZ-Pn-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filter Data for Alerts\n",
        "1. Filter data (check for conditions):\n",
        "    Condition:\n",
        "    * Glucose Reading > 115 OR\n",
        "    * Rolling Window Average of 10 readings > 105\n",
        "\n",
        "2. Send filtered data to special Eventhub"
      ],
      "metadata": {
        "id": "3c9VbBHpXLq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the window for rolling average calculation, partitioned by patient_id\n",
        "windowSpec = Window.partitionBy(\"patient_id\").orderBy(\"timestamp\").rowsBetween(-9, 0)\n",
        "\n",
        "# Calculate the rolling average and count of readings per patient\n",
        "df_with_rolling_avg = df.withColumn(\"rolling_avg\", avg(\"glucose_level\").over(windowSpec)) \\\n",
        "                        .withColumn(\"reading_count\", count(\"glucose_level\").over(windowSpec))\n",
        "\n",
        "# Filter based on the conditions\n",
        "filtered_df = df_with_rolling_avg.filter(((col(\"glucose_level\") > 115) | (col(\"rolling_avg\") > 105)) & (col(\"reading_count\") >= 10))\n"
      ],
      "metadata": {
        "id": "V7acfzdvP5j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#send data as json\n",
        "\n",
        "from azure.eventhub import EventHubProducerClient, EventData\n",
        "\n",
        "connection_string = CONNECTION_STRING\n",
        "\n",
        "def send_to_eventhub_batch(batch_df, batch_id):\n",
        "    if not batch_df.rdd.isEmpty():\n",
        "        producer = EventHubProducerClient.from_connection_string(\n",
        "            conn_str=connection_string,\n",
        "            eventhub_name=\"alerts\"\n",
        "        )\n",
        "        with producer:\n",
        "            event_data_batch = producer.create_batch()\n",
        "            for row in batch_df.collect():\n",
        "                event_data = EventData(str(row.asDict()))\n",
        "                try:\n",
        "                    # Add the event to the batch\n",
        "                    event_data_batch.add(event_data)\n",
        "                except ValueError:\n",
        "                    # The batch is full, send it and start a new batch\n",
        "                    producer.send_batch(event_data_batch)\n",
        "                    event_data_batch = producer.create_batch()\n",
        "                    event_data_batch.add(event_data)  # Add the event to the new batch\n",
        "            # Send any remaining events in the batch\n",
        "            if len(event_data_batch) > 0:\n",
        "                producer.send_batch(event_data_batch)\n",
        "\n",
        "\n",
        "# Streaming query (treating the static DataFrame as a stream)\n",
        "query = filtered_df.writeStream \\\n",
        "    .foreachBatch(send_to_eventhub_batch) \\\n",
        "    .start()\\\n",
        "    .awaitTermination()"
      ],
      "metadata": {
        "id": "Z3gFJ_Cpb867"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}