{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generate + Filter data in the same File"
      ],
      "metadata": {
        "id": "3aQ9Yb_PTz0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark_release='spark-3.4.2'\n",
        "hadoop_version='hadoop3'\n",
        "\n",
        "import os, time\n",
        "start=time.time()\n",
        "os.environ['SPARK_RELEASE']=spark_release\n",
        "os.environ['HADOOP_VERSION']=hadoop_version\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_release}-bin-{hadoop_version}\""
      ],
      "metadata": {
        "id": "ZgAT-64nTzf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker pysqlite3\n",
        "!pip install mysql.connector\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "-2DQkwDEbV-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b59bd92e-7ef4-4628-a47e-1eeba3a8db61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faker in /usr/local/lib/python3.10/dist-packages (22.0.0)\n",
            "Requirement already satisfied: pysqlite3 in /usr/local/lib/python3.10/dist-packages (0.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
            "Requirement already satisfied: mysql.connector in /usr/local/lib/python3.10/dist-packages (2.2.9)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark"
      ],
      "metadata": {
        "id": "bnJ-4Q8NyMNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run below commands in google colab\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # install Java8\n",
        "!wget -q http://apache.osuosl.org/spark/${SPARK_RELEASE}/${SPARK_RELEASE}-bin-${HADOOP_VERSION}.tgz # download spark-3.3.X\n",
        "!tar xf ${SPARK_RELEASE}-bin-${HADOOP_VERSION}.tgz # unzip it\n",
        "!pip install -q findspark # install findspark"
      ],
      "metadata": {
        "id": "wl8IziHQycfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "import multiprocessing\n",
        "import pyspark\n",
        "import socket\n",
        "import uuid\n",
        "import findspark\n",
        "from pyspark.conf import SparkConf\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import expr\n",
        "from pyspark.sql.streaming import DataStreamReader\n",
        "\n",
        "import sqlite3\n",
        "from faker import Faker\n",
        "import random\n",
        "import datetime\n",
        "import json\n",
        "fake=Faker()"
      ],
      "metadata": {
        "id": "0ednyWm-bt52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# findspark find your Spark Distribution and sets necessary environment variables\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "tc2xPTOJyee0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the pyspark version\n",
        "print(pyspark.__version__)"
      ],
      "metadata": {
        "id": "7h-gacbeygMT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ebdfe8c-1ddd-42e0-f4a7-2c5a5f35a1a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DStream that will connect to hostname:port, like localhost:9999\n",
        "# if doing this over a network, firewalls may block the connection!\n",
        "hostname=socket.gethostname()\n",
        "\n",
        "hostname"
      ],
      "metadata": {
        "id": "N1G55x3fymTE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58ca0c24-b03f-4c7f-b693-f127a671ee06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'05a220fe72d3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "app_id=str(uuid.uuid1())\n",
        "\n",
        "app_id"
      ],
      "metadata": {
        "id": "qjL2UCxoyoBy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82feb13e-6f90-4d4e-83e7-d7436876b7a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2173ace4-a9bd-11ee-9fbc-0242ac1c000c'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conf = SparkConf()\n",
        "\n",
        "conf.setAll([\n",
        "     ('spark.app.name', app_id),\n",
        "     ('spark.shuffle.useOldFetchProtocol', 'true'),\n",
        "     ('spark.testing', 'true'), # Avoid minimum 450M executor/driver memory https://www.waitingforcode.com/apache-spark/troubleshooting-system-memory-must-be-at-least-error/read / https://programmerclick.com/article/72821685476/\n",
        "     ('spark.driver.allowMultipleContexts','true'), # https://stackoverflow.com/a/41591258 This option is used only for Spark internal tests and is not to be used in production.\n",
        "     ('spark. y', '100M'),\n",
        "     # ('spark.driver.memory ', '200M'),\n",
        "     # ('spark.executor.instances',1), # This property is no longer used in Spark 2+\n",
        "     # number of executors is determined as: floor(spark.cores.max / spark.executor.cores)\n",
        "     (\"spark.executor.cores\",1), # cores per executor. https://stackoverflow.com/questions/39399205/spark-standalone-number-executors-cores-control/39400195#39400195\n",
        "     (\"spark.cores.max\", 2), # the maximum amount of CPU cores to request for the application from across the cluster (not from each machine)\n",
        "     ('spark.submit.deployMode', 'client'), # client, cluster\n",
        "     ('spark.ui.showConsoleProgress', 'true'),\n",
        "     (\"fs.azure\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\") ])"
      ],
      "metadata": {
        "id": "8LlJ3BHFyogz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9882af1b-2d51-41b1-91a0-c2efdbe8c031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.conf.SparkConf at 0x7eca804c7490>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "end=time.time()\n",
        "\n",
        "\n",
        "f'Spark setup time: {int(end-start)} seconds'"
      ],
      "metadata": {
        "id": "_J85b_QsysNI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "855ffc94-923c-41b1-cf17-d0caf4c845f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Spark setup time: 115 seconds'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip mysql-connector-j-8.2.0.zip"
      ],
      "metadata": {
        "id": "yg-xyAr7IiG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set up directories for later usage\n",
        "!mkdir glucose_readings_dir device_readings_dir alerts_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRCbpQLG1WYH",
        "outputId": "6c760abc-392d-4266-bcaa-c3265f7887d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘glucose_readings_dir’: File exists\n",
            "mkdir: cannot create directory ‘device_readings_dir’: File exists\n",
            "mkdir: cannot create directory ‘alerts_dir’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Data\n",
        "1. Take patient data stored in MySQL (e.g. ID)\n",
        "2. Generate Glucse Readings Data — use timesynth library for better time-dependency of health data\n",
        "3. Generate Device Readings Data — use Faker"
      ],
      "metadata": {
        "id": "UDCHiW5AiZFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mysql.connector\n",
        "\n",
        "# MySQL database credentials\n",
        "host= DB_HOST\n",
        "port= DB_PORT\n",
        "username= DB_USERNAME\n",
        "password= DB_PASSWORD\n",
        "database= DB_NAME\n",
        "\n",
        "\n",
        "# Establish a connection to the MySQL database\n",
        "cnx = mysql.connector.connect(user=username, password=password,\n",
        "                              host=host, database=database)\n",
        "\n",
        "cursor = cnx.cursor()\n",
        "sql_str='SELECT COUNT(*) FROM Patient;'\n",
        "rs=cursor.execute(sql_str)\n",
        "rs=cursor.fetchall()\n",
        "print(rs)\n",
        "\n",
        "# Fetch existing device IDs\n",
        "cursor.execute(\"SELECT device_id FROM DeviceStaticInfo\")\n",
        "device_ids = [row[0] for row in cursor.fetchall()]\n",
        "print(device_ids)\n",
        "\n",
        "cursor.execute(\"SELECT patient_id FROM Patient\")\n",
        "patient_ids = [row[0] for row in cursor.fetchall()]\n",
        "print(patient_ids)"
      ],
      "metadata": {
        "id": "OSA7Ip_lX816",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffa6d305-08a2-4e97-9edf-a9a3e011efe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(20,)]\n",
            "[1, 2, 5, 3, 4]\n",
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_device_readings(iterations, device_ids):\n",
        "  # Function to generate random firmware version\n",
        "  def random_firmware_version():\n",
        "      major = random.randint(1, 3)\n",
        "      minor = random.randint(0, 9)\n",
        "      patch = random.randint(0, 9)\n",
        "      return f\"{major}.{minor}.{patch}\"\n",
        "\n",
        "  # Function to generate random connectivity status\n",
        "  def random_connectivity_status():\n",
        "      return random.choice(['Connected', 'Disconnected', 'Poor Connection'])\n",
        "\n",
        "  # Function to generate random error codes\n",
        "  def random_error_codes():\n",
        "      if random.choice([True, False]):\n",
        "          return random.choice(['Err1', 'Err2', 'Err3', ''])\n",
        "      return ''\n",
        "\n",
        "  # List to store generated operational data\n",
        "  operational_data = []\n",
        "\n",
        "  # Generate and add device operational data to the list\n",
        "  for i in range(iterations):\n",
        "    for device_id in device_ids:\n",
        "        for _ in range(1):  # Generate 1 record per device\n",
        "            data = {\n",
        "                'device_id': device_id,\n",
        "                'timestamp': fake.date_time_between(start_date=\"-30d\", end_date=\"now\").strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                'battery_level': random.randint(0, 100),  # Battery level from 0 to 100%\n",
        "                'firmware_version': random_firmware_version(),\n",
        "                'connectivity_status': random_connectivity_status(),\n",
        "                'error_codes': random_error_codes()\n",
        "            }\n",
        "            operational_data.append(data)\n",
        "\n",
        "    # Write the generated operational data to a JSON file\n",
        "    with open(f'./device_readings_dir/device_operational_data_{datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S_%f\")}.json', 'w') as file:\n",
        "        json.dump(operational_data, file, indent=4)\n",
        "        print(f\"Generated {len(operational_data)} device operational data records and saved them to '{file.name}'\")\n",
        "\n",
        "    #clear\n",
        "    operational_data=[]"
      ],
      "metadata": {
        "id": "7RO-GcEdiYSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/TimeSynth/TimeSynth.git\n",
        "%cd TimeSynth\n",
        "!pip install .\n",
        "!pip install timesynth\n",
        "%cd ..\n",
        "#Then restart runtime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMWsotk904cV",
        "outputId": "166d2a33-f4b9-46e3-f4e3-83468159406a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'TimeSynth' already exists and is not an empty directory.\n",
            "/content/TimeSynth\n",
            "Processing /content/TimeSynth\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from timesynth==0.2.4) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from timesynth==0.2.4) (1.11.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from timesynth==0.2.4) (1.12)\n",
            "Requirement already satisfied: symengine>=0.4 in /usr/local/lib/python3.10/dist-packages (from timesynth==0.2.4) (0.11.0)\n",
            "Requirement already satisfied: jitcdde==1.4 in /usr/local/lib/python3.10/dist-packages (from timesynth==0.2.4) (1.4.0)\n",
            "Requirement already satisfied: jitcxde_common==1.4.1 in /usr/local/lib/python3.10/dist-packages (from timesynth==0.2.4) (1.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from jitcxde_common==1.4.1->timesynth==0.2.4) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from jitcxde_common==1.4.1->timesynth==0.2.4) (67.7.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->timesynth==0.2.4) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->jitcxde_common==1.4.1->timesynth==0.2.4) (2.1.3)\n",
            "Building wheels for collected packages: timesynth\n",
            "  Building wheel for timesynth (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for timesynth: filename=timesynth-0.2.4-py3-none-any.whl size=15422 sha256=12b55bf7ef3132bee314233acee5e48afa559bb510b5033f6b8ff89ca777cb8d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dfk3s13e/wheels/81/84/c6/880288cb791ed65eb7343a49a4918038d6e3bf7622d3d187e0\n",
            "Successfully built timesynth\n",
            "Installing collected packages: timesynth\n",
            "  Attempting uninstall: timesynth\n",
            "    Found existing installation: timesynth 0.2.4\n",
            "    Uninstalling timesynth-0.2.4:\n",
            "      Successfully uninstalled timesynth-0.2.4\n",
            "Successfully installed timesynth-0.2.4\n",
            "Requirement already satisfied: timesynth in /usr/local/lib/python3.10/dist-packages (0.2.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from timesynth) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from timesynth) (1.11.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from timesynth) (1.12)\n",
            "Requirement already satisfied: symengine>=0.4 in /usr/local/lib/python3.10/dist-packages (from timesynth) (0.11.0)\n",
            "Requirement already satisfied: jitcdde==1.4 in /usr/local/lib/python3.10/dist-packages (from timesynth) (1.4.0)\n",
            "Requirement already satisfied: jitcxde-common==1.4.1 in /usr/local/lib/python3.10/dist-packages (from timesynth) (1.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from jitcxde-common==1.4.1->timesynth) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from jitcxde-common==1.4.1->timesynth) (67.7.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->timesynth) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->jitcxde-common==1.4.1->timesynth) (2.1.3)\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import timesynth as ts\n",
        "import numpy as np\n",
        "import random\n",
        "from faker import Faker\n",
        "\n",
        "\n",
        "def generate_glucose_readings(iterations, patient_ids, device_ids):\n",
        "    # Define user profiles\n",
        "    user_profiles = ['diabetic', 'athlete', 'party-goer', 'low-glucose', 'elderly']\n",
        "\n",
        "    # Map patient IDs to user profiles\n",
        "    patient_profile_map = {patient_id: random.choice(user_profiles) for patient_id in patient_ids}\n",
        "\n",
        "    # TimeSynth setup\n",
        "    time_sampler = ts.TimeSampler(stop_time=1)\n",
        "    regular_time_samples = time_sampler.sample_regular_time(num_points=1)\n",
        "    sinusoid = ts.signals.Sinusoidal(frequency=0.25)\n",
        "    white_noise = ts.noise.GaussianNoise(std=0.3)\n",
        "    timeseries = ts.TimeSeries(sinusoid, noise_generator=white_noise)\n",
        "\n",
        "    # Glucose reading generator based on user profile\n",
        "    def generate_glucose_reading(profile):\n",
        "      samples, _, _ = timeseries.sample(regular_time_samples)\n",
        "\n",
        "      if profile == 'diabetic':\n",
        "          return samples * 40 + 130  # Higher readings\n",
        "      elif profile == 'athlete':\n",
        "          return samples * 20 + 90   # Lower post-exercise readings\n",
        "      elif profile == 'party-goer':\n",
        "          return samples * 50 + 130  # Variable readings\n",
        "      elif profile == 'low-glucose':\n",
        "          return samples * 15 + 55   # Dangerously low readings\n",
        "      elif profile == 'elderly':\n",
        "          return samples * 35 + 145  # Steady/higher readings\n",
        "      else:\n",
        "          return samples * 40 + 110  # Normal range for other patients\n",
        "\n",
        "\n",
        "    # List to store generated readings\n",
        "    readings = []\n",
        "\n",
        "    # Generate glucose readings\n",
        "    for i in range(iterations):\n",
        "        for patient_id in patient_ids:\n",
        "            profile = patient_profile_map[patient_id]\n",
        "            glucose_levels = generate_glucose_reading(profile)\n",
        "\n",
        "            for glucose_level in glucose_levels:\n",
        "\n",
        "                reading = {\n",
        "                    'patient_id': patient_id,\n",
        "                    'device_id': random.choice(device_ids),\n",
        "                    'glucose_level': round(glucose_level, 2),\n",
        "                    'timestamp': fake.date_time_between(start_date=\"-30d\", end_date=\"now\").strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                    'location': fake.city()\n",
        "                }\n",
        "                readings.append(reading)\n",
        "\n",
        "        # Write the generated readings to a JSON file\n",
        "        with open(f'./glucose_readings_dir/glucose_readings_corrected_{datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S_%f\")}.json', 'w') as file:\n",
        "            json.dump(readings, file, indent=4)\n",
        "            print(f\"Generated {iterations} glucose reading(s) for each patient and saved them to '{file.name}'\")\n",
        "\n",
        "        readings = []\n"
      ],
      "metadata": {
        "id": "m-hq2S81-IFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate Device Data: 1 reading per device — 1 json file will be generated with all readings\n",
        "#by default we have 5 devices\n",
        "generate_device_readings(1, device_ids)\n",
        "\n",
        "#Generate Glucse Readings: 1 sample for each patient — 1 json file will be generated with all readings\n",
        "#by default we have 20 patients\n",
        "generate_glucose_readings(15, patient_ids, device_ids )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncLdmoUj04qe",
        "outputId": "0917d637-1dd8-4012-826d-499712c9b2d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 5 device operational data records and saved them to './device_readings_dir/device_operational_data_2024_01_02_22_16_23_182121.json'\n",
            "Generated 15 glucose reading(s) for each patient and saved them to './glucose_readings_dir/glucose_readings_corrected_2024_01_02_22_16_23_202632.json'\n",
            "Generated 15 glucose reading(s) for each patient and saved them to './glucose_readings_dir/glucose_readings_corrected_2024_01_02_22_16_23_212910.json'\n",
            "Generated 15 glucose reading(s) for each patient and saved them to './glucose_readings_dir/glucose_readings_corrected_2024_01_02_22_16_23_221518.json'\n",
            "Generated 15 glucose reading(s) for each patient and saved them to './glucose_readings_dir/glucose_readings_corrected_2024_01_02_22_16_23_230911.json'\n",
            "Generated 15 glucose reading(s) for each patient and saved them to './glucose_readings_dir/glucose_readings_corrected_2024_01_02_22_16_23_240336.json'\n",
            "Generated 15 glucose reading(s) for each patient and saved them to './glucose_readings_dir/glucose_readings_corrected_2024_01_02_22_16_23_251549.json'\n",
            "Generated 15 glucose reading(s) for each patient and saved them to './glucose_readings_dir/glucose_readings_corrected_2024_01_02_22_16_23_262628.json'\n",
            "Generated 15 glucose reading(s) for each patient and saved them to './glucose_readings_dir/glucose_readings_corrected_2024_01_02_22_16_23_270082.json'\n",
            "Generated 15 glucose reading(s) for each patient and saved them to './glucose_readings_dir/glucose_readings_corrected_2024_01_02_22_16_23_278475.json'\n",
            "Generated 15 glucose reading(s) for each patient and saved them to './glucose_readings_dir/glucose_readings_corrected_2024_01_02_22_16_23_287999.json'\n",
            "Generated 15 glucose reading(s) for each patient and saved them to './glucose_readings_dir/glucose_readings_corrected_2024_01_02_22_16_23_299057.json'\n",
            "Generated 15 glucose reading(s) for each patient and saved them to './glucose_readings_dir/glucose_readings_corrected_2024_01_02_22_16_23_311507.json'\n",
            "Generated 15 glucose reading(s) for each patient and saved them to './glucose_readings_dir/glucose_readings_corrected_2024_01_02_22_16_23_319939.json'\n",
            "Generated 15 glucose reading(s) for each patient and saved them to './glucose_readings_dir/glucose_readings_corrected_2024_01_02_22_16_23_331029.json'\n",
            "Generated 15 glucose reading(s) for each patient and saved them to './glucose_readings_dir/glucose_readings_corrected_2024_01_02_22_16_23_340720.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to generate data in the background:"
      ],
      "metadata": {
        "id": "5hIDErt59TBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a process to run the generate_data function in the background\n",
        "device_readings_process = multiprocessing.Process(target=generate_device_readings, args=(1, device_ids))\n",
        "glucose_readings_process = multiprocessing.Process(target=generate_glucose_readings, args=(1, patient_ids, device_ids))\n",
        "\n",
        "# Start the process\n",
        "device_readings_process.start()\n",
        "glucose_readings_process.start()"
      ],
      "metadata": {
        "id": "IzrAJ4KfcHNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show Generated Data"
      ],
      "metadata": {
        "id": "dl9NALqAAKOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Opening JSON file\n",
        "f = open('glucose_readings_dir/glucose_readings_corrected_2024_01_02_11_42_06_426773.json')\n",
        "\n",
        "# returns JSON object as\n",
        "# a dictionary\n",
        "data = json.load(f)\n",
        "\n",
        "# Iterating through the json\n",
        "# list\n",
        "for i in data:\n",
        "    print(i)\n",
        "\n",
        "# Closing file\n",
        "f.close()"
      ],
      "metadata": {
        "id": "S8nj8PXflFeu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12092fbd-4837-46c7-b882-84f8c01f656e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'patient_id': 1, 'device_id': 3, 'glucose_level': 107.58, 'timestamp': '2023-12-23 04:04:09', 'location': 'South John'}\n",
            "{'patient_id': 2, 'device_id': 2, 'glucose_level': 119.24, 'timestamp': '2023-12-22 16:22:07', 'location': 'Reneeland'}\n",
            "{'patient_id': 3, 'device_id': 2, 'glucose_level': 128.09, 'timestamp': '2023-12-08 11:26:49', 'location': 'Morgantown'}\n",
            "{'patient_id': 4, 'device_id': 4, 'glucose_level': 147.17, 'timestamp': '2023-12-12 23:09:57', 'location': 'North Martinstad'}\n",
            "{'patient_id': 5, 'device_id': 4, 'glucose_level': 135.7, 'timestamp': '2023-12-26 21:58:35', 'location': 'North Vincentmouth'}\n",
            "{'patient_id': 6, 'device_id': 4, 'glucose_level': 153.98, 'timestamp': '2023-12-10 08:22:53', 'location': 'Christophershire'}\n",
            "{'patient_id': 7, 'device_id': 1, 'glucose_level': 134.78, 'timestamp': '2023-12-19 12:37:24', 'location': 'West Sarah'}\n",
            "{'patient_id': 8, 'device_id': 5, 'glucose_level': 126.55, 'timestamp': '2023-12-11 20:08:11', 'location': 'Shannonland'}\n",
            "{'patient_id': 9, 'device_id': 1, 'glucose_level': 109.62, 'timestamp': '2023-12-28 07:51:25', 'location': 'Vegashire'}\n",
            "{'patient_id': 10, 'device_id': 4, 'glucose_level': 77.62, 'timestamp': '2023-12-12 07:01:55', 'location': 'East Alicia'}\n",
            "{'patient_id': 11, 'device_id': 1, 'glucose_level': 126.56, 'timestamp': '2023-12-18 19:18:52', 'location': 'Connerfurt'}\n",
            "{'patient_id': 12, 'device_id': 3, 'glucose_level': 155.93, 'timestamp': '2023-12-31 20:11:21', 'location': 'Michelletown'}\n",
            "{'patient_id': 13, 'device_id': 5, 'glucose_level': 159.19, 'timestamp': '2023-12-16 16:58:47', 'location': 'Alexistown'}\n",
            "{'patient_id': 14, 'device_id': 5, 'glucose_level': 135.56, 'timestamp': '2023-12-29 16:13:02', 'location': 'South Diane'}\n",
            "{'patient_id': 15, 'device_id': 2, 'glucose_level': 142.38, 'timestamp': '2023-12-29 01:28:17', 'location': 'Lake Sara'}\n",
            "{'patient_id': 16, 'device_id': 1, 'glucose_level': 82.54, 'timestamp': '2023-12-28 09:01:11', 'location': 'Katherinehaven'}\n",
            "{'patient_id': 17, 'device_id': 1, 'glucose_level': 156.68, 'timestamp': '2023-12-08 01:43:56', 'location': 'Christineberg'}\n",
            "{'patient_id': 18, 'device_id': 1, 'glucose_level': 96.63, 'timestamp': '2023-12-21 18:46:00', 'location': 'Michelleside'}\n",
            "{'patient_id': 19, 'device_id': 2, 'glucose_level': 134.4, 'timestamp': '2023-12-31 18:29:19', 'location': 'Torresbury'}\n",
            "{'patient_id': 20, 'device_id': 1, 'glucose_level': 143.02, 'timestamp': '2023-12-06 10:43:01', 'location': 'Lake Markland'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Opening JSON file\n",
        "f = open('/content/device_readings_dir/device_operational_data_2024_01_02_11_46_16_130197.json')\n",
        "\n",
        "# returns JSON object as\n",
        "# a dictionary\n",
        "data = json.load(f)\n",
        "\n",
        "# Iterating through the json\n",
        "# list\n",
        "for i in data:\n",
        "    print(i)\n",
        "\n",
        "# Closing file\n",
        "f.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIneAZRkAQpg",
        "outputId": "313383f0-30db-457f-a6f4-6b2342984373"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'device_id': 1, 'timestamp': '2023-12-30 08:09:24', 'battery_level': 57, 'firmware_version': '1.3.7', 'connectivity_status': 'Disconnected', 'error_codes': ''}\n",
            "{'device_id': 2, 'timestamp': '2023-12-17 18:18:04', 'battery_level': 55, 'firmware_version': '2.7.2', 'connectivity_status': 'Poor Connection', 'error_codes': ''}\n",
            "{'device_id': 5, 'timestamp': '2023-12-13 02:24:01', 'battery_level': 15, 'firmware_version': '2.9.0', 'connectivity_status': 'Disconnected', 'error_codes': ''}\n",
            "{'device_id': 3, 'timestamp': '2024-01-02 05:18:39', 'battery_level': 68, 'firmware_version': '3.8.3', 'connectivity_status': 'Poor Connection', 'error_codes': ''}\n",
            "{'device_id': 4, 'timestamp': '2023-12-08 20:35:51', 'battery_level': 79, 'firmware_version': '1.6.1', 'connectivity_status': 'Disconnected', 'error_codes': ''}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Send Data to Eventhub"
      ],
      "metadata": {
        "id": "t1gHLO60AZG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install azure.eventhub"
      ],
      "metadata": {
        "id": "0RB2dII0ApQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"JSONtoEventHub\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# schema definitions\n",
        "glucose_schema = StructType([\n",
        "    StructField(\"patient_id\", IntegerType(), True),\n",
        "    StructField(\"device_id\", IntegerType(), True),\n",
        "    StructField(\"glucose_level\", FloatType(), True),\n",
        "    StructField(\"timestamp\", TimestampType(), True),\n",
        "    StructField(\"location\", StringType(), True)\n",
        "])\n",
        "\n",
        "device_schema = StructType([\n",
        "    StructField(\"device_id\", IntegerType(), True),\n",
        "    StructField(\"timestamp\", TimestampType(), True),\n",
        "    StructField(\"battery_level\", FloatType(), True),\n",
        "    StructField(\"firmware_version\", StringType(), True),\n",
        "    StructField(\"connectivity_status\", StringType(), True),\n",
        "    StructField(\"error_codes\", StringType(), True)\n",
        "])\n",
        "\n",
        "\n",
        "# Read JSON file into DataFrame\n",
        "json_df_glucose = spark.readStream.option('multiline', True).schema(glucose_schema).json(\"./glucose_readings_dir/\")\n",
        "json_df_device = spark.readStream.option('multiline', True).schema(device_schema).json(\"./device_readings_dir/\")\n",
        "#.option('multiline', True)"
      ],
      "metadata": {
        "id": "G4Nm3-JeApQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.eventhub import EventHubProducerClient, EventData\n",
        "\n",
        "connection_string = YOUR_CONNECTION_STRING\n",
        "\n",
        "def send_to_eventhub_batch(batch_df, batch_id):\n",
        "    if not batch_df.rdd.isEmpty():\n",
        "        producer = EventHubProducerClient.from_connection_string(\n",
        "            conn_str=connection_string,\n",
        "            eventhub_name=\"glucose_monitoring\"\n",
        "        )\n",
        "        with producer:\n",
        "            event_data_batch = producer.create_batch()\n",
        "            for row in batch_df.collect():\n",
        "                event_data = EventData(str(row.asDict()))\n",
        "                try:\n",
        "                    # Add the event to the batch\n",
        "                    event_data_batch.add(event_data)\n",
        "                except ValueError:\n",
        "                    # The batch is full, send it and start a new batch\n",
        "                    producer.send_batch(event_data_batch)\n",
        "                    event_data_batch = producer.create_batch()\n",
        "                    event_data_batch.add(event_data)  # Add the event to the new batch\n",
        "            # Send any remaining events in the batch\n",
        "            if len(event_data_batch) > 0:\n",
        "                producer.send_batch(event_data_batch)\n",
        "\n",
        "\n",
        "# Streaming query (treating the static DataFrame as a stream)\n",
        "query = json_df_glucose.writeStream \\\n",
        "    .foreachBatch(send_to_eventhub_batch) \\\n",
        "    .start()\\\n",
        "    .awaitTermination()"
      ],
      "metadata": {
        "id": "6FG4PNNjApQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.eventhub import EventHubProducerClient, EventData\n",
        "\n",
        "connection_string = YOUR_CONNECTION_STRING\n",
        "\n",
        "def send_to_eventhub_batch(batch_df, batch_id):\n",
        "    if not batch_df.rdd.isEmpty():\n",
        "        producer = EventHubProducerClient.from_connection_string(\n",
        "            conn_str=connection_string,\n",
        "            eventhub_name=\"device_readings\"\n",
        "        )\n",
        "        with producer:\n",
        "            event_data_batch = producer.create_batch()\n",
        "            for row in batch_df.collect():\n",
        "                event_data = EventData(str(row.asDict()))\n",
        "                try:\n",
        "                    # Add the event to the batch\n",
        "                    event_data_batch.add(event_data)\n",
        "                except ValueError:\n",
        "                    # The batch is full, send it and start a new batch\n",
        "                    producer.send_batch(event_data_batch)\n",
        "                    event_data_batch = producer.create_batch()\n",
        "                    event_data_batch.add(event_data)  # Add the event to the new batch\n",
        "            # Send any remaining events in the batch\n",
        "            if len(event_data_batch) > 0:\n",
        "                producer.send_batch(event_data_batch)\n",
        "\n",
        "\n",
        "# Streaming query (treating the static DataFrame as a stream)\n",
        "query = json_df_device.writeStream \\\n",
        "    .foreachBatch(send_to_eventhub_batch) \\\n",
        "    .start()\\\n",
        "    .awaitTermination()"
      ],
      "metadata": {
        "id": "ctgI5x5VCFEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bwqj4cr5CFJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) Filter Data for Alerts\n",
        "1. Condition: (Glucose Reading > 115) OR (Rolling Window Average of 10 readings > 105)\n",
        "2. Send filtered data to Eventhub\n",
        "3. From Eventhub, Azure Functions picks data and send alert to Telegram channel.\n",
        "\n",
        "\n",
        "How Code works:\n",
        "1. Read the streaming data from JSON files.\n",
        "2. Use window functions to calculate the rolling average. Since the files are in chronological order, make sure the files are ingested by the streaming job in the same order.\n",
        "3. Apply the filter based on the rolling average and the individual glucose level readings.\n",
        "\n",
        "\n",
        "This code will calculate the rolling average for the last 10 glucose readings for each patient, and it assumes that each file is treated as a separate micro-batch in chronological order. If a patient has fewer than 10 readings in the latest micro-batch, the average will be calculated over however many readings are available.\n",
        "\n"
      ],
      "metadata": {
        "id": "eCj82oe4GID6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, explode\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType, ArrayType\n",
        "from azure.eventhub import EventHubProducerClient, EventData\n",
        "connection_string = YOUR_CONNECTION_STRING\n",
        "\n",
        "\n",
        "# Initialize Spark Session for Streaming\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"GlucoseStreamingAnalysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Define schema for individual glucose reading\n",
        "glucose_reading_schema = StructType([\n",
        "    StructField(\"patient_id\", IntegerType(), True),\n",
        "    StructField(\"device_id\", IntegerType(), True),\n",
        "    StructField(\"glucose_level\", FloatType(), True),\n",
        "    StructField(\"timestamp\", TimestampType(), True),\n",
        "    StructField(\"location\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Read streaming data from JSON files\n",
        "json_df_glucose = spark.readStream \\\n",
        "    .option(\"multiLine\", True) \\\n",
        "    .schema(StructType([StructField(\"readings\", ArrayType(glucose_reading_schema))])) \\\n",
        "    .json(\"glucose_readings_dir/\") \\\n",
        "    .select(explode(col(\"readings\")).alias(\"reading\")) \\\n",
        "    .select(\"reading.*\")\n",
        "\n",
        "# Filter records where glucose level is greater than 115\n",
        "filtered_glucose_df = json_df_glucose.filter(col(\"glucose_level\") > 115)\n",
        "\n",
        "\n",
        "def send_to_eventhub_batch(batch_df, batch_id):\n",
        "    if not batch_df.rdd.isEmpty():\n",
        "        producer = EventHubProducerClient.from_connection_string(\n",
        "            conn_str=connection_string,\n",
        "            eventhub_name=\"alerts\"\n",
        "        )\n",
        "        with producer:\n",
        "            event_data_batch = producer.create_batch()\n",
        "            for row in batch_df.collect():\n",
        "                event_data = EventData(str(row.asDict()))\n",
        "                try:\n",
        "                    # Add the event to the batch\n",
        "                    event_data_batch.add(event_data)\n",
        "                except ValueError:\n",
        "                    # The batch is full, send it and start a new batch\n",
        "                    producer.send_batch(event_data_batch)\n",
        "                    event_data_batch = producer.create_batch()\n",
        "                    event_data_batch.add(event_data)  # Add the event to the new batch\n",
        "            # Send any remaining events in the batch\n",
        "            if len(event_data_batch) > 0:\n",
        "                producer.send_batch(event_data_batch)\n",
        "\n",
        "# Streaming query (treating the static DataFrame as a stream)\n",
        "query = filtered_glucose_df.writeStream \\\n",
        "    .foreachBatch(send_to_eventhub_batch) \\\n",
        "    .start()\\\n",
        "    .awaitTermination()"
      ],
      "metadata": {
        "id": "lPNcvPcUCFMC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}